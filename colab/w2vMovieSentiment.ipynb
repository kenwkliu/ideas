{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w2vMovieSentiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh-ZjcWTM0_N"
      },
      "source": [
        "# Movie sentiment analysis\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%load_ext google.colab.data_table \n",
        "\n",
        "!wget https://raw.githubusercontent.com/kenwkliu/ideas/master/colab/preprocess.py\n",
        "import preprocess\n",
        "from string import punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqNFoPHLV-Yc"
      },
      "source": [
        "# Load all files from a directory in a DataFrame.\n",
        "def load_directory_data(directory):\n",
        "  data = {}\n",
        "  data[\"sentence\"] = []\n",
        "  data[\"sentiment\"] = []\n",
        "  for file_path in os.listdir(directory):\n",
        "    with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "      data[\"sentence\"].append(f.read())\n",
        "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(directory):\n",
        "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "  pos_df[\"polarity\"] = 1\n",
        "  neg_df[\"polarity\"] = 0\n",
        "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Download and process the dataset files.\n",
        "def download_and_load_datasets(force_download=False):\n",
        "  dataset = tf.keras.utils.get_file(\n",
        "      fname=\"aclImdb.tar.gz\", \n",
        "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "      extract=True)\n",
        "  \n",
        "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                       \"aclImdb\", \"train\"))\n",
        "  \n",
        "  return train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4MBFmCZNwP6"
      },
      "source": [
        "# Download the movie review dataset\n",
        "SHOW = 5\n",
        "train = download_and_load_datasets()\n",
        "\n",
        "print('Total training rows:', len(train))\n",
        "train[:SHOW]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgDib6Csh9GG"
      },
      "source": [
        "# Preprocess the reviews which include remove symbols and stop words, to small cap, stem the words\r\n",
        "train['preprocessed'] = train['sentence'].apply(preprocess.process)\r\n",
        "train[:SHOW]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzLg51r8OuxF"
      },
      "source": [
        "# define input_x, input_y\n",
        "train_text = train['preprocessed'].values\n",
        "train_label = train['polarity'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mgr4MdbPIUJ"
      },
      "source": [
        "index = 0\n",
        "print('input_x:', train_text[index])\n",
        "print('input_y:', train_label[index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0t1l49VPdTZ"
      },
      "source": [
        "# Neural Network only understands vector of numbers\n",
        "# Need to represent the input word sequence as vector of numbers for the Neural Network input\n",
        "# Use the word occurency frequency in the dataset as the word's indexed number\n",
        "# For simplicity, only consider the top frequent 10,000 words in the dataset (assume more frequent is more important)\n",
        "max_words = 10000 \n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_text)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens but will only consider the top frequent %s words' %(len(word_index), max_words))\n",
        "\n",
        "# convert the words to a seqence of numbers (index by the top frequency of the words)\n",
        "# The words in the text sequence will be indexed as a number by the word_index\n",
        "sequences = tokenizer.texts_to_sequences(train_text)\n",
        "\n",
        "# show the word_index\n",
        "df = pd.DataFrame.from_dict(word_index, orient='index', columns=['number'])\n",
        "df[:max_words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUpZO1WD6B7I"
      },
      "source": [
        "# Check the convert results\n",
        "index = 0\n",
        "\n",
        "print(train_text[index])\n",
        "print(sequences[index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Cj213YR6HcE"
      },
      "source": [
        "# We will cut reviews after maxlen words because neural network input is fixed size vector\n",
        "maxlen = 200 \n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = np.asarray(train_label)\n",
        "\n",
        "print('Shape of data tensor', data.shape)\n",
        "print('Shape of labels tensor', labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgC-brkz_h-8"
      },
      "source": [
        "# Now the training X, Y is the acceptable format by Neural Network\n",
        "# Input X is a vectors and Y is the corresponding labels (i.e. the sentiment)\n",
        "index = 0\n",
        "print(data[index])\n",
        "print(\"\\n lable:\", labels[index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qieRWfVQEGC"
      },
      "source": [
        "training_samples = 16000  \n",
        "validation_samples = 4000  \n",
        "test_samples = 5000\n",
        "\n",
        "# Split the data into a training set and a validation set\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]\n",
        "x_test = data[training_samples + validation_samples:]\n",
        "y_test = labels[training_samples + validation_samples:]\n",
        "\n",
        "test_text = train_text[training_samples + validation_samples:]\n",
        "test_label = train_label[training_samples + validation_samples:]\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"x_val shape:\", x_val.shape)\n",
        "print(\"y_val shape:\", y_val.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT6qWIWExf4_"
      },
      "source": [
        "# Use the one hidden Dense layer connection which is similar to the MNIST (Digit) recognition\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(32, activation='relu', input_shape=(maxlen,)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "plot_model(model, show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# [good movie ....] --word_indexed_input--> [9,2,...,200] --dense--> [1,2,.....,32] --dense_output--> 0 or 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XooLKaW9yiAH"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP-iKsFsyod1"
      },
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))\n",
        "\n",
        "# Plot the accuracy\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT5OCfejknal"
      },
      "source": [
        "# As usual, the neural network will learn the input/output associations (i.e. map the sentence (i.e. text sequence numberrs) to the sentiment)\r\n",
        "# However, in order to do the mapping effectively, neural network needs to understand the text \r\n",
        "# (which are now merely encoded as numbers by the frequency of appearance in the movies reviews) \r\n",
        "# So the challenge is that the Neural Network has to learn 2 things (input/output mapping and language understanding) at the same time\r\n",
        "# Comparing to the image recognition, the pixel positions are naturally mapped to the vectors to represent the image.\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hnuRAp_U6jg"
      },
      "source": [
        "# The key idea here is to use word2vec (a pretrained vector which understand the text to certain extend) \r\n",
        "# to solve the 2nd issue (text understanding) and helps the neural network to learn the input/output mappings.\r\n",
        "\r\n",
        "from gensim.models import word2vec\r\n",
        "\r\n",
        "reviews = [sent.split(' ') for sent in train['preprocessed']]\r\n",
        "w2vModel = word2vec.Word2Vec(reviews, min_count=5, size=64, window=5)\r\n",
        "w2v = w2vModel.wv\r\n",
        "\r\n",
        "print(\"No.of words in the vocab:\", len(w2v.vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwZxYgwmWLfl"
      },
      "source": [
        "w2v.most_similar('movie')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoC6Ew_G2zxD"
      },
      "source": [
        "# build the embedding matrix from the word2vec that we will be able to load into an Embedding layer. \n",
        "# where each entry i contains the word2vec vector \n",
        "# for the word of index i in our reference word index (built during earlier tokenization step). \n",
        "\n",
        "# Note that the index 0 is not supposed to stand for any word or token -- it's a placeholder.\n",
        "# The Embedding layer a matrix of shape (max_words, embedding_dim), \n",
        "\n",
        "embedding_dim = w2v.vector_size\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "  # Words not found in embedding index will be all-zeros.\n",
        "  if word in w2v.vocab and i < max_words:\n",
        "    embedding_matrix[i] = w2v[word]\n",
        "\n",
        "print(\"embedding_matrix.shape:\", embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKEzktwg5J13"
      },
      "source": [
        "# Use w2v embedding layer which encode the input data more meaningfully\n",
        "# Then flatten the embedding output to a single vector for the hidden Dense layer \n",
        "\n",
        "w2vModel = Sequential()\n",
        "w2vModel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "# flatten the 200 sequence of 300-dim w2v vector into one single vector for dense layer\n",
        "w2vModel.add(Flatten())\n",
        "w2vModel.add(Dense(32, activation='relu'))\n",
        "w2vModel.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "plot_model(w2vModel, show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# [good movie ....] --word_indexed_input--> [9,2,...,200] --w2v_embedding--> [[1,3,...,4], [7,5,...19],...] \n",
        "#   --flatten--> [1,3,...4,7,5,...19,...] --dense--> [1,2,.....,32] --dense_output--> 0 or 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCy5i84YYIAL"
      },
      "source": [
        "w2vModel.summary()\n",
        "\n",
        "# Embedding layer = max_words * embedding_dim = 10,000 * 300 = 3,000,000\n",
        "# Flatten = (input) maxlen * embedding_dim = 200 * 300 = 60,000\n",
        "# dense = flatten_output * dense_output_neurons + bias = 60,000 * 32 + 32 = 1,920,032\n",
        "# dense_1 = dense_output * dense_1_output + bias = 32 * 1 + 1 = 33"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6CNp_e45WxZ"
      },
      "source": [
        "# Load the Embedding layer from the embedding_matrix and no need to train the parameters \n",
        "\n",
        "w2vModel.layers[0].set_weights([embedding_matrix])\n",
        "w2vModel.layers[0].trainable = False\n",
        "w2vModel.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr5VNegd51kf"
      },
      "source": [
        "# compile the model after setting the Embedding layer to non-trainable\n",
        "w2vModel.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = w2vModel.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))\n",
        "\n",
        "# Plot the accuracy\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIH8M-nV6RE9"
      },
      "source": [
        "# Do the prediction on the test set\n",
        "score = w2vModel.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "ans = w2vModel.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpU7zTBG6p1O"
      },
      "source": [
        "# Predict individual movie review\n",
        "p_index = 0\n",
        "\n",
        "print(\"Movie Review:\", test_text[p_index])\n",
        "print(\"Truth Sentiment:\", test_label[p_index])\n",
        "print(\"Predicted Sentiment:\", round(ans[p_index][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xia7838u1Jxk"
      },
      "source": [
        "# Change the neural network architecture from dense layer to LSTM which is better for sequence \n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "lstm = Sequential()\n",
        "lstm.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "# No need to flatten the vector (required for dense layer but not for LSTM)\n",
        "lstm.add(LSTM(32))\n",
        "lstm.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Load the Embedding layer from the embedding_matrix and no need to train the parameters \n",
        "lstm.layers[0].set_weights([embedding_matrix])\n",
        "lstm.layers[0].trainable = False\n",
        "\n",
        "lstm.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "plot_model(lstm, show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# [good movie ....] --word_indexed_input--> [9,2,..,200] --w2v_embedding--> [[1,3,...,4], [7,5,...19],...] \n",
        "#   --LSTM--> [[1,3,...,4], [7,5,...19],...] --dense_output--> 0 or 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gofvoc_52Z1B"
      },
      "source": [
        "lstm.summary()\n",
        "# The lstm (LSTM) Param # = g * [h(h+i) + h]\n",
        "# where g is number of gates and LSTM = 4\n",
        "# h = no. of LSTM hidden neurons\n",
        "# i = the dimension of input (feature)\n",
        "# 4 * (32 (32 + 300) + 32) = 42624"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmgd2hHp2z4_"
      },
      "source": [
        "history = lstm.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))\n",
        "\n",
        "# Plot the accuracy\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfHaA-CnAdSW"
      },
      "source": [
        "# Suggested changes\n",
        "# - Try with different max_words and maxlen in the data processing\n",
        "# - When padding the text sequence to fixed size vector, removing some portion from the middle \n",
        "#       because usually the first and last sentence are more important as far as sentiment is concerned\n",
        "# - Tune the neural network hyper-parameters\n",
        "# - Train with other dataset such news sentiment dataset"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}