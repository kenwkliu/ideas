{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w2vMovieSentiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh-ZjcWTM0_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Movie sentiment analysis\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqNFoPHLV-Yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load all files from a directory in a DataFrame.\n",
        "def load_directory_data(directory):\n",
        "  data = {}\n",
        "  data[\"sentence\"] = []\n",
        "  data[\"sentiment\"] = []\n",
        "  for file_path in os.listdir(directory):\n",
        "    with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "      data[\"sentence\"].append(f.read())\n",
        "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(directory):\n",
        "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "  pos_df[\"polarity\"] = 1\n",
        "  neg_df[\"polarity\"] = 0\n",
        "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Download and process the dataset files.\n",
        "def download_and_load_datasets(force_download=False):\n",
        "  dataset = tf.keras.utils.get_file(\n",
        "      fname=\"aclImdb.tar.gz\", \n",
        "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "      extract=True)\n",
        "  \n",
        "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                       \"aclImdb\", \"train\"))\n",
        "  \n",
        "  return train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4MBFmCZNwP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = download_and_load_datasets()\n",
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzLg51r8OuxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text = train['sentence'].values\n",
        "train_label = train['polarity'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mgr4MdbPIUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = 2\n",
        "print(train_text[index])\n",
        "print(train_label[index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0t1l49VPdTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_words = 10000  # We will only consider the top frequent 10,000 words in the dataset\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_text)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens but will only consider the top frequent %s words' %(len(word_index), max_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn32oL4j5TM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUpZO1WD6B7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert the words to a seqence of numbers (index by the top frequency of the words)\n",
        "sequences = tokenizer.texts_to_sequences(train_text)\n",
        "\n",
        "index = 3\n",
        "print(train_text[index])\n",
        "print(sequences[index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Cj213YR6HcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will cut reviews after maxlen words because neural network input is fixed size vector\n",
        "maxlen = 200 \n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = np.asarray(train_label)\n",
        "\n",
        "print('Shape of data tensor', data.shape)\n",
        "print('Shape of labels tensor', labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgC-brkz_h-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now the training X, Y is the acceptable format by Neural Network\n",
        "# Input X is a vectors and Y is the corresponding labels (i.e. the sentiment)\n",
        "index = 3\n",
        "print(data[index])\n",
        "print(\"\\n lable:\", labels[index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qieRWfVQEGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_samples = 16000  \n",
        "validation_samples = 4000  \n",
        "test_samples = 5000\n",
        "\n",
        "# Split the data into a training set and a validation set\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]\n",
        "x_test = data[training_samples + validation_samples:]\n",
        "y_test = labels[training_samples + validation_samples:]\n",
        "\n",
        "test_text = train_text[training_samples + validation_samples:]\n",
        "test_label = train_label[training_samples + validation_samples:]\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"x_val shape:\", x_val.shape)\n",
        "print(\"y_val shape:\", y_val.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT6qWIWExf4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the one hidden Dense layer connection which is similar to the MNIST (Digit) recognition\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(32, activation='relu', input_shape=(maxlen,)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "plot_model(model, show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XooLKaW9yiAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP-iKsFsyod1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))\n",
        "\n",
        "# Plot the accuracy\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GqOoIi61BqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# As usual, the neural network will learn the input/output associations (i.e. map the sentence sequence to the sentiment)\n",
        "# In order to do the mapping effectively, neural network needs to understand the text \n",
        "# (which are now merely encoded as numbers by the frequency) \n",
        "# So the challenge is that the Neural Network has to learn 2 things (input/output mapping and language understanding) at the same time\n",
        "# Comparing to the image recognition, the pixel positions are naturally mapped to the vectors to represent the image.\n",
        "\n",
        "# The key idea here is to use word2vec (a pretrained vector which understand the text to certain extend) \n",
        "# to solve the 2nd issue (text understanding) and helps the neural network to learn the input/output mappings.\n",
        "\n",
        "#load a pre-trainied Google News w2v\n",
        "import nltk\n",
        "nltk.download('word2vec_sample')\n",
        "\n",
        "from nltk.data import find\n",
        "import gensim\n",
        "\n",
        "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
        "w2v = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)\n",
        "\n",
        "#Each word is represented in the space of 300 dimensions:\n",
        "embedding_dim = w2v.vector_size\n",
        "\n",
        "print(\"No.of words in the vocab:\", len(w2v.vocab))\n",
        "print(\"Model dimensions:\", embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZrA68iZ2p6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testWord = 'boy'\n",
        "w2v[testWord]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoC6Ew_G2zxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the embedding matrix from the word2vec that we will be able to load into an Embedding layer. \n",
        "# It must be a matrix of shape (max_words, embedding_dim), \n",
        "# where each entry i contains the word2vec vector \n",
        "# for the word of index i in our reference word index (built during tokenization). \n",
        "# Note that the index 0 is not supposed to stand for any word or token -- it's a placeholder.\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "  # Words not found in embedding index will be all-zeros.\n",
        "  if word in w2v.vocab and i < max_words:\n",
        "    embedding_matrix[i] = w2v[word]\n",
        "\n",
        "print(\"embedding_matrix.shape:\", embedding_matrix.shape)\n",
        "print(\"max_words:\", max_words)\n",
        "print(\"embedding_dim:\", embedding_dim)\n",
        "print(\"maxlen:\", maxlen)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7E1qS8d4KCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check this vector is the same as the previous w2v vector\n",
        "wIndex = word_index[testWord]\n",
        "embedding_matrix[wIndex]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKEzktwg5J13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use w2v embedding layer which encode the input data more meaningfully\n",
        "# Then flatten the embedding output to a single vector for the hidden Dense layer \n",
        "\n",
        "w2vModel = Sequential()\n",
        "w2vModel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "# flatten the 200 sequence of 300-dim w2v vector into one single vector for dense layer\n",
        "w2vModel.add(Flatten())\n",
        "w2vModel.add(Dense(32, activation='relu'))\n",
        "w2vModel.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "plot_model(w2vModel, show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCy5i84YYIAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2vModel.summary()\n",
        "\n",
        "# Embedding layer = max_words * embedding_dim = 10,000 * 300 = 3,000,000\n",
        "# Flatten = (input) maxlen * embedding_dim = 200 * 300 = 60,000\n",
        "# dense = flatten_output * dense_output_neurons + bias = 60,000 * 32 + 32 = 1,920,032\n",
        "# dense_1 = dense_output * dense_1_output + bias = 32 * 1 + 1 = 33"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6CNp_e45WxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the Embedding layer from the embedding_matrix and no need to train the parameters \n",
        "\n",
        "w2vModel.layers[0].set_weights([embedding_matrix])\n",
        "w2vModel.layers[0].trainable = False\n",
        "w2vModel.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr5VNegd51kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile the model after setting the Embedding layer to non-trainable\n",
        "w2vModel.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = w2vModel.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))\n",
        "\n",
        "# Plot the accuracy\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIH8M-nV6RE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Do the prediction on the test set\n",
        "score = w2vModel.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "ans = w2vModel.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpU7zTBG6p1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict individual movie review\n",
        "p_index = 20\n",
        "\n",
        "print(\"Movie Review:\", test_text[p_index])\n",
        "print(\"Truth Sentiment:\", test_label[p_index])\n",
        "print(\"Predicted Sentiment:\", round(ans[p_index][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xia7838u1Jxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change the neural network architecture from dense layer to LSTM which is better for sequence \n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "lstm = Sequential()\n",
        "lstm.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "# No need to flatten the vector (required for dense layer but not for LSTM)\n",
        "lstm.add(LSTM(32))\n",
        "lstm.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Load the Embedding layer from the embedding_matrix and no need to train the parameters \n",
        "lstm.layers[0].set_weights([embedding_matrix])\n",
        "lstm.layers[0].trainable = False\n",
        "\n",
        "lstm.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "plot_model(lstm, show_shapes=True, show_layer_names=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gofvoc_52Z1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm.summary()\n",
        "# The lstm (LSTM) Param # = g * [h(h+i) + h]\n",
        "# where g is number of gates and LSTM = 4\n",
        "# h = no. of LSTM hidden neurons\n",
        "# i = the dimension of input (feature)\n",
        "# 4 * (32 (32 + 300) + 32) = 42624"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmgd2hHp2z4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = lstm.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))\n",
        "\n",
        "# Plot the accuracy\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfHaA-CnAdSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Suggested changes\n",
        "# - Instead of using Google News word2vec, use the Movie reivew itself to train the word2vec to embed here\n",
        "# - Preprocess the train_text such as removing the stop words\n",
        "# - Try with different max_words and maxlen in the data processing\n",
        "# - When padding the text sequence to fixed size vector, removing some portion from the middle \n",
        "#       because usually the first and last sentence are more important as far as sentiment is concerned\n",
        "# - Tune the neural network hyper-parameters\n",
        "# - Train with News Sentiment dataset "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}