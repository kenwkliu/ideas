{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"w2vMovieSentiment.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"Dh-ZjcWTM0_N","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","import os\n","import re\n","\n","# Load all files from a directory in a DataFrame.\n","def load_directory_data(directory):\n","  data = {}\n","  data[\"sentence\"] = []\n","  data[\"sentiment\"] = []\n","  for file_path in os.listdir(directory):\n","    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n","      data[\"sentence\"].append(f.read())\n","      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n","  return pd.DataFrame.from_dict(data)\n","\n","# Merge positive and negative examples, add a polarity column and shuffle.\n","def load_dataset(directory):\n","  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n","  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n","  pos_df[\"polarity\"] = 1\n","  neg_df[\"polarity\"] = 0\n","  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n","\n","# Download and process the dataset files.\n","def download_and_load_datasets(force_download=False):\n","  dataset = tf.keras.utils.get_file(\n","      fname=\"aclImdb.tar.gz\", \n","      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n","      extract=True)\n","  \n","  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n","                                       \"aclImdb\", \"train\"))\n","  \n","  return train_df\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"V4MBFmCZNwP6","colab_type":"code","colab":{}},"cell_type":"code","source":["train = download_and_load_datasets()\n","train"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LzLg51r8OuxF","colab_type":"code","colab":{}},"cell_type":"code","source":["train_text = train['sentence'].values\n","train_label = train['polarity'].values"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4Mgr4MdbPIUJ","colab_type":"code","colab":{}},"cell_type":"code","source":["index = 3\n","print(train_text[index])\n","print(train_label[index])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c0t1l49VPdTZ","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","\n","max_words = 10000  # We will only consider the top 10,000 words in the dataset\n","\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(train_text)\n","sequences = tokenizer.texts_to_sequences(train_text)\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","maxlen = 200 # We will cut reviews after maxlen words\n","data = pad_sequences(sequences, maxlen=maxlen)\n","labels = np.asarray(train_label)\n","\n","print('Shape of data tensor', data.shape)\n","print('Shape of labels tensor', labels.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1qieRWfVQEGC","colab_type":"code","colab":{}},"cell_type":"code","source":["training_samples = 16000  \n","validation_samples = 4000  \n","test_samples = 5000\n","\n","# Split the data into a training set and a validation set\n","x_train = data[:training_samples]\n","y_train = labels[:training_samples]\n","x_val = data[training_samples: training_samples + validation_samples]\n","y_val = labels[training_samples: training_samples + validation_samples]\n","x_test = data[training_samples + validation_samples:]\n","y_test = labels[training_samples + validation_samples:]\n","\n","test_text = train_text[training_samples + validation_samples:]\n","test_label = train_label[training_samples + validation_samples:]\n","\n","print(\"x_train shape:\", x_train.shape)\n","print(\"y_train shape:\", y_train.shape)\n","print(\"x_val shape:\", x_val.shape)\n","print(\"y_val shape:\", y_val.shape)\n","print(\"x_test shape:\", x_test.shape)\n","print(\"y_test shape:\", y_test.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9GqOoIi61BqR","colab_type":"code","colab":{}},"cell_type":"code","source":["#load a pre-trainied Google News w2v\n","\n","!pip install nltk\n","!pip install gensim\n","\n","import nltk\n","nltk.download('word2vec_sample')\n","\n","from nltk.data import find\n","import gensim\n","\n","word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n","w2v = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)\n","\n","#Each word is represented in the space of 300 dimensions:\n","embedding_dim = w2v.vector_size\n","\n","print(\"No.of words in the vocab:\", len(w2v.vocab))\n","print(\"Model dimensions:\", embedding_dim)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FZrA68iZ2p6-","colab_type":"code","colab":{}},"cell_type":"code","source":["testWord = 'boy'\n","w2v[testWord]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IoC6Ew_G2zxD","colab_type":"code","colab":{}},"cell_type":"code","source":["# build the embedding matrix that we will be able to load into an Embedding layer. \n","# It must be a matrix of shape (max_words,embedding_dim), \n","# where each entry i contains the embedding_dim-dimensional vector for the word of index i in our reference word index \n","# (built during tokenization). Note that the index 0 is not supposed to stand for any word or token -- it's a placeholder.\n","\n","embedding_matrix = np.zeros((max_words, embedding_dim))\n","for word, i in word_index.items():\n","  # Words not found in embedding index will be all-zeros.\n","  if word in w2v.vocab and i < max_words:\n","    embedding_vector = w2v[word]\n","    embedding_matrix[i] = embedding_vector"],"execution_count":0,"outputs":[]},{"metadata":{"id":"D7E1qS8d4KCj","colab_type":"code","colab":{}},"cell_type":"code","source":["# check this vector is the same as the previous w2v vector\n","wIndex = word_index[testWord]\n","embedding_matrix[wIndex]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RieC3zH-4gQK","colab_type":"code","colab":{}},"cell_type":"code","source":["print(\"embedding_matrix.shape:\", embedding_matrix.shape)\n","print(\"max_words:\", max_words)\n","print(\"embedding_dim:\", embedding_dim)\n","print(\"maxlen:\", maxlen)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SKEzktwg5J13","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Embedding, Flatten, Dense\n","\n","model = Sequential()\n","model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n","model.add(Flatten())\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.summary()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"U6CNp_e45WxZ","colab_type":"code","colab":{}},"cell_type":"code","source":["# Load the Embedding layer from the embedding_matrix and no need to train the parameters\n","\n","model.layers[0].set_weights([embedding_matrix])\n","model.layers[0].trainable = False\n","model.summary()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Yr5VNegd51kf","colab_type":"code","colab":{}},"cell_type":"code","source":["model.compile(optimizer='rmsprop',\n","              loss='binary_crossentropy',\n","              metrics=['acc'])\n","history = model.fit(x_train, y_train,\n","                    epochs=10,\n","                    batch_size=32,\n","                    validation_data=(x_val, y_val))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wIH8M-nV6RE9","colab_type":"code","colab":{}},"cell_type":"code","source":["# Do the prediction on the test set\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])\n","\n","ans = model.predict(x_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FpU7zTBG6p1O","colab_type":"code","colab":{}},"cell_type":"code","source":["# Predict individual movie review\n","p_index = 15\n","\n","print(\"Movie Review:\", test_text[p_index])\n","print(\"Truth Sentiment:\", test_label[p_index])\n","print(\"Predicted Sentiment:\", round(ans[p_index][0]))"],"execution_count":0,"outputs":[]}]}