{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "preprocess.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSb-_6hddD1h"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/kenwkliu/ideas/master/colab/preprocess.py\n",
        "import preprocess\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "\n",
        "def plotWordCloud(wordcloud):\n",
        "  plt.figure(figsize=[15,10])\n",
        "  plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1rVxQS4dD1n"
      },
      "source": [
        "text = \"How are you, Tom? How many works do you have to work?\"\n",
        "print(preprocess._removeSymbols(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IULY3OAadD1q"
      },
      "source": [
        "text = \"How are you, Tom? How many works do you have to work?\"\n",
        "print(text.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdNkymvRdD1s"
      },
      "source": [
        "text = \"How are you, Tom? How many works do you have to work?\"\n",
        "print(preprocess._stem(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta9VtFZ7JNVg"
      },
      "source": [
        "# Stemming by PorterStemmer\n",
        "# https://www.nltk.org/_modules/nltk/stem/porter.html\n",
        "\n",
        "s = PorterStemmer()\n",
        "print(s.stem('Having'))\n",
        "print(s.stem('Have'))\n",
        "print(s.stem('Had'))\n",
        "\n",
        "print(s.stem('Fishing'))\n",
        "print(s.stem('Fish'))\n",
        "print(s.stem('Fisher'))\n",
        "print(s.stem('Fishes'))\n",
        "print(s.stem('Fished'))\n",
        "\n",
        "print(s.stem('am'))\n",
        "print(s.stem('is'))\n",
        "print(s.stem('was'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0aghCz-JPwA"
      },
      "source": [
        "# Lemmatization by WordNet\n",
        "# Lemmatization is the process of converting a word to its base form. \n",
        "# Lemmatization considers the context and converts the word to its meaningful base form, \n",
        "# whereas stemming just removes the last few characters\n",
        "# Sometimes the same word can have multiple different lemmas. \n",
        "# Based on the context (by POS tag), extract the appropriate lemma.\n",
        "\n",
        "s = WordNetLemmatizer()\n",
        "print(s.lemmatize('having', pos='v'))\n",
        "print(s.lemmatize('have', pos='v'))\n",
        "print(s.lemmatize('had', pos='v'))\n",
        "\n",
        "print(s.lemmatize('fishing', pos='v'))\n",
        "print(s.lemmatize('fish', pos='v'))\n",
        "print(s.lemmatize('fisher', pos='n'))\n",
        "print(s.lemmatize('fishes', pos='v'))\n",
        "print(s.lemmatize('fished', pos='v'))\n",
        "\n",
        "print(s.lemmatize('am', pos='v'))\n",
        "print(s.lemmatize('is', pos='v'))\n",
        "print(s.lemmatize('was', pos='v'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOV78KWadD1v"
      },
      "source": [
        "text = \"How are you, Tom? How many works do you have to work?\"\n",
        "print(preprocess._stop(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s9JF5vldQUq"
      },
      "source": [
        "preprocess.stop_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSZRvismdD1y"
      },
      "source": [
        "text = \"How are you, Tom? How many works do you have to work?\"\n",
        "print(preprocess.process(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNt9OcwDdD10"
      },
      "source": [
        "# Segment Chinese sentence into words\n",
        "import jieba\n",
        "\n",
        "seg_list = jieba.cut(\"兒子生性病母倍感安慰\")\n",
        "print(\"/\".join(seg_list))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu-DjWpin0E8"
      },
      "source": [
        "seg_list = jieba.cut(\"白石角新發展區位於沙田與大埔之間，2012年起發展成住宅區，多個樓盤如天賦海灣、逸瓏灣、海日灣及朗濤等相繼落成及入伙\")\r\n",
        "print(\"/\".join(seg_list))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C801UbtNlZxK"
      },
      "source": [
        "seg_list = jieba.cut(\"雷军称不送充电器创意是他首创的，不是抄苹果\")\r\n",
        "print(\"/\".join(seg_list)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4DIvI6wmRin"
      },
      "source": [
        "# Install the cantonese\r\n",
        "!pip install pycantonese"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kVD9LwJmaAL"
      },
      "source": [
        "import pycantonese as pc\r\n",
        "\r\n",
        "pc.segment(\"兒子生性病母倍感安慰\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyQwf52fdpCz"
      },
      "source": [
        "# Combine English words to phrases\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "\n",
        "documents = [\n",
        "    \"the cheif executive officer of new york was there\", \n",
        "    \"machine learning can be useful sometimes\",\n",
        "    \"new york cheif executive officer was present\",\n",
        "    \"machine learning is good\"\n",
        "]\n",
        "\n",
        "sentence_stream = [doc.split(\" \") for doc in documents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY7mW2WmdxSI"
      },
      "source": [
        "bigram = Phraser(Phrases(sentence_stream, min_count=1, threshold=2))\n",
        "\n",
        "for sent in bigram[sentence_stream]:\n",
        "    print(sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcRb1OQmd1yC"
      },
      "source": [
        "trigram = Phraser(Phrases(bigram[sentence_stream], min_count=1, threshold=2))\n",
        "\n",
        "for sent in trigram[bigram[sentence_stream]]:\n",
        "    print(sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7itH9NuwwuhZ"
      },
      "source": [
        "# Generate word cloud from Apple Tweets\n",
        "url = 'https://raw.githubusercontent.com/kenwkliu/ideas/master/colab/data/appleTweets.xlsx'\n",
        "appleTweets = pd.read_excel(url)\n",
        "appleTweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EACNhZj2fKB"
      },
      "source": [
        "text = \" \".join(words for words in appleTweets['Tweet content'])\n",
        "text[:5000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lxdVJaadUrT"
      },
      "source": [
        "# Word cloud\n",
        "MAX_WORDS = 50\n",
        "WIDTH, HEIGHT = 800, 600\n",
        "BG_COLOR = \"black\" # white\n",
        "\n",
        "plotWordCloud(WordCloud(max_words=MAX_WORDS, width=WIDTH, height=HEIGHT, stopwords='', background_color=BG_COLOR).generate(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cOgYjo53SW-"
      },
      "source": [
        "appleTweets['cleaned'] = appleTweets['Tweet content'].apply(preprocess.process)\n",
        "appleTweets[['Tweet content', 'cleaned']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoST7i_U4BUn"
      },
      "source": [
        "cleaned_text = \" \".join(words for words in appleTweets['cleaned'])\n",
        "cleaned_text[:5000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QexLTvLf4Oq8"
      },
      "source": [
        "# application level stop words\n",
        "STOP_WORDS = ['apple', 'aapl', 'http', 'co', 'inc', 'read', 'ha']\n",
        "cleanedCloud = WordCloud(max_words=MAX_WORDS, width=WIDTH, height=HEIGHT, stopwords=STOP_WORDS, background_color=BG_COLOR).generate(cleaned_text)\n",
        "plotWordCloud(cleanedCloud)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy0Edl0-LtWd"
      },
      "source": [
        "# Save the word cloud to a file\n",
        "cleanedCloud.to_file(\"wordcloud.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRMGqKI97Ymu"
      },
      "source": [
        "# https://www.datacamp.com/community/tutorials/wordcloud-python\n",
        "?WordCloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZWNCwan9kNd"
      },
      "source": [
        "# Remove the unsupported characters and save to a file\n",
        "appleTweets = appleTweets.applymap(lambda x: x.encode('unicode_escape').decode('utf-8') if isinstance(x, str) else x)\n",
        "appleTweets.to_excel('appleTweetsCleaned.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}