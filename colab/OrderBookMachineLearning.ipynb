{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DecisionTree.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHkyqgSAZk9U"
      },
      "source": [
        "# Use Machine Learning to predict short-term stock price movement\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#scikit learn Machine Learning library\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# interative plotting library\n",
        "import altair as alt\n",
        "\n",
        "# Google colab interactive table\n",
        "%load_ext google.colab.data_table \n",
        "\n",
        "SCREEN_X, SCREEN_Y = 12, 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2g1sCBook0L"
      },
      "source": [
        "# From the raw Market Tick Data, construct Order Book\n",
        "# Generates the features dataset from Order Book\n",
        "\n",
        "# Import order book features dataset of 0005.HK in 20131002\n",
        "# It contains L1, L2, L3 order_book_imbalance and MF (money_flow) to predict the next stock price movement (u=up or d=down)\n",
        "# The data is normalized to the range of -1 to 1\n",
        "# Please refer to course note for the order book features explanation\n",
        "orderBook = pd.read_csv(\"https://raw.githubusercontent.com/kenwkliu/ideas/master/colab/data/20131002-5HK-OrderBookFeatures.csv\")\n",
        "orderBook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buQT3vJGSicP"
      },
      "source": [
        "# Scatter plot all the order book features \n",
        "sns.pairplot(orderBook, hue=\"Dir\",palette=\"bright\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlpddri1I4Of"
      },
      "source": [
        "# Compare 2 order features interactively\n",
        "alt.Chart(orderBook).mark_point().encode(\n",
        "  x='L1',\n",
        "  y='L2',\n",
        "  color='Dir'\n",
        ").properties(\n",
        "  width=640,\n",
        "  height=480\n",
        ").interactive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuvlxUyMotE7"
      },
      "source": [
        "# split dataset in features and target variable\n",
        "# which means using the features to predict the target\n",
        "feature_cols = ['L1', 'L2', 'L3', 'MF']\n",
        "X = orderBook[feature_cols] # Features\n",
        "y = orderBook.Dir # Target variable\n",
        "\n",
        "print(\"Features:\", X.columns)\n",
        "print(\"Target:\", y.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ausY7QOXfaXZ"
      },
      "source": [
        "# Split dataset into training set and test set\n",
        "# 80% training and 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "print(\"Training feature size:\", X_train.shape)\n",
        "print(\"Testing feature size:\", X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzIUHugDrWqn"
      },
      "source": [
        "# https://www.datacamp.com/community/tutorials/decision-tree-classification-python\n",
        "from sklearn import tree\n",
        "\n",
        "# Create Decision Tree classifer object with maxiumn 3 level (max_depth=3)\n",
        "# max_depth is the hyperparamter that determine model capacity \n",
        "dtree = tree.DecisionTreeClassifier(max_depth=3)\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "dtree = dtree.fit(X_train,y_train)\n",
        "\n",
        "print(\"Decision Tree training completed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvvqw9SPZ4da"
      },
      "source": [
        "# Visualize the decision tree\n",
        "import graphviz \n",
        "dot_data = tree.export_graphviz(dtree, out_file=None) \n",
        "graph = graphviz.Source(dot_data) \n",
        "graph.render(\"imbal\") \n",
        "\n",
        "dot_data = tree.export_graphviz(dtree, out_file=None, \n",
        "                      feature_names=feature_cols,  \n",
        "                      class_names=[\"D\", \"U\"],  \n",
        "                      filled=True, rounded=True,  \n",
        "                      special_characters=True)  \n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph \n",
        "\n",
        "\n",
        "# gini: quantifies the purity of the node/leaf. \n",
        "# A gini score greater than zero implies that samples contained within that node belong to different classes. \n",
        "# A gini score of zero means that the node is pure, that within that node only a single class of samples exist. \n",
        " \n",
        "# value: how many samples in each category (Down vs Up in this case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6Q4kGJRrc6H"
      },
      "source": [
        "# Measure the Decision tree performance\n",
        "y_dtree_pred_train = dtree.predict(X_train)\n",
        "y_dtree_pred_test = dtree.predict(X_test)\n",
        "\n",
        "print(\"Train Accuracy:\", metrics.accuracy_score(y_train, y_dtree_pred_train))\n",
        "print(\"Test Accuracy:\", metrics.accuracy_score(y_test, y_dtree_pred_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6yuHSaC22Nb"
      },
      "source": [
        "# Plot confusion matrix on the test set\n",
        "plot_confusion_matrix(dtree, X_test, y_test, display_labels=orderBook.Dir, cmap=plt.cm.Blues)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hh9OZRfTuJk4"
      },
      "source": [
        "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# use SVM with default settings\n",
        "svm = SVC()\n",
        "svm.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ-zrwGHubPV"
      },
      "source": [
        "# Measure the SVM performance\n",
        "y_svm_pred_train = svm.predict(X_train)\n",
        "y_svm_pred_test = svm.predict(X_test)\n",
        "\n",
        "print(\"Train Accuracy:\", metrics.accuracy_score(y_train, y_svm_pred_train))\n",
        "print(\"Test Accuracy:\", metrics.accuracy_score(y_test, y_svm_pred_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWLjg_hE7MLg"
      },
      "source": [
        "# Plot confusion matrix on the test set\n",
        "plot_confusion_matrix(svm, X_test, y_test, display_labels=orderBook.Dir, cmap=plt.cm.Blues)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtgHcDC1Dchk"
      },
      "source": [
        "# Scatter plot the L1, L2, L3 order_book_imbalance features\n",
        "# They have similar patterns and can be compressed\n",
        "\n",
        "sns.pairplot(orderBook[['L1', 'L2', 'L3', 'Dir']], hue=\"Dir\",palette=\"bright\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cIf6ws__pbJ"
      },
      "source": [
        "# If the training time is too long because the input dimension is too high, \n",
        "# can use PCA to compressed the data features to fewer dimensions\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Compress the L1, L2, L3 order_book_imbalance features into one 'principal component' by PCA\n",
        "pca = PCA(n_components=1)\n",
        "principalComponents = pca.fit_transform(orderBook[['L1', 'L2', 'L3']])\n",
        "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component'])\n",
        "principalDf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_ROGqUVFDGk"
      },
      "source": [
        "# Combine back the MF (moneyflow) and the price direction\n",
        "principalOrderBook = pd.concat([principalDf, orderBook[['MF', 'Dir']]], axis = 1)\n",
        "principalOrderBook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wne9S-jBfvD1"
      },
      "source": [
        "# Plot 'principal component' vs 'money flow'\n",
        "alt.Chart(principalOrderBook).mark_point().encode(x='principal component', y='MF', color='Dir').properties(width=320, height=240).interactive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xi0423Cghkc"
      },
      "source": [
        "# Plot imbalance vs 'money flow'\n",
        "alt.Chart(orderBook).mark_point().encode(x='L1', y='MF', color='Dir').properties(width=320, height=240).interactive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STv3-nUfA3k5"
      },
      "source": [
        "# split dataset in features and target variable\n",
        "# which means using the features to predict the target\n",
        "feature_cols = ['principal component', 'MF']\n",
        "pX = principalOrderBook[feature_cols] # Features\n",
        "py = principalOrderBook.Dir # Target variable\n",
        "\n",
        "print(\"Features:\", pX.columns)\n",
        "print(\"Target:\", py.name)\n",
        "\n",
        "# Split dataset into training set and test set\n",
        "# 80% training and 20% test\n",
        "pX_train, pX_test, py_train, py_test = train_test_split(pX, py, test_size=0.2, random_state=1)\n",
        "\n",
        "print(\"Training feature size:\", pX_train.shape)\n",
        "print(\"Testing feature size:\", pX_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDL6nYs7CdcN"
      },
      "source": [
        "# Train the principle feature by decision tree and measure the performance\n",
        "pDtree = tree.DecisionTreeClassifier(max_depth=3).fit(pX_train, py_train)\n",
        "py_dtree_pred_train = pDtree.predict(pX_train)\n",
        "py_dtree_pred_test = pDtree.predict(pX_test)\n",
        "\n",
        "# accuracy with the PCA feature\n",
        "print(\"PCA Train Accuracy:\", metrics.accuracy_score(py_train, py_dtree_pred_train))\n",
        "print(\"PCA Test Accuracy:\", metrics.accuracy_score(py_test, py_dtree_pred_test))\n",
        "\n",
        "# accuracy without the PCA feature\n",
        "print(\"Train Accuracy:\", metrics.accuracy_score(y_train, y_dtree_pred_train))\n",
        "print(\"Test Accuracy:\", metrics.accuracy_score(y_test, y_dtree_pred_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rb_d6bWVBnM7"
      },
      "source": [
        "# Train the principle feature by SVM and measure the performance\n",
        "pSvm = SVC().fit(pX_train, py_train)\n",
        "py_svm_pred_train = pSvm.predict(pX_train)\n",
        "py_svm_pred_test = pSvm.predict(pX_test)\n",
        "\n",
        "print(\"PCA Train Accuracy:\", metrics.accuracy_score(py_train, py_svm_pred_train))\n",
        "print(\"PCA Test Accuracy:\", metrics.accuracy_score(py_test, py_svm_pred_test))\n",
        "\n",
        "print(\"Train Accuracy:\", metrics.accuracy_score(y_train, y_svm_pred_train))\n",
        "print(\"Test Accuracy:\", metrics.accuracy_score(y_test, y_svm_pred_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9byBeMJOayC"
      },
      "source": [
        "# Partitioning the available data into three sets (train, validation, test) \n",
        "# will drastically reduce the number of samples which can be used for learning the model,\n",
        "# and the results can depend on a particular random choice for the pair of (train, validation) sets.\n",
        "# Use cross-validation (CV): a test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. \n",
        "# https://scikit-learn.org/stable/modules/cross_validation.html\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(dtree, X_train, y_train, cv=5)\n",
        "print(\"Decision Tree Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
        "print(scores)\n",
        "\n",
        "scores = cross_val_score(svm, X_train, y_train, cv=5)\n",
        "print(\"SVM Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
        "print(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA-IdJNtdr1O"
      },
      "source": [
        "# Hyper-parameters optimization using grid search with cross-validation\n",
        "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
        "\n",
        "# Set the parameters by cross-validation\n",
        "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
        "                     'C': [1, 10, 100, 1000]},\n",
        "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
        "\n",
        "scores = ['precision', 'recall']\n",
        "\n",
        "for score in scores:\n",
        "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
        "    print()\n",
        "\n",
        "    clf = GridSearchCV(\n",
        "        SVC(), tuned_parameters, scoring='%s_macro' % score\n",
        "    )\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Best parameters set found on development set:\")\n",
        "    print()\n",
        "    print(clf.best_params_)\n",
        "    print()\n",
        "    print(\"Grid scores on development set:\")\n",
        "    print()\n",
        "    means = clf.cv_results_['mean_test_score']\n",
        "    stds = clf.cv_results_['std_test_score']\n",
        "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
        "              % (mean, std * 2, params))\n",
        "    print()\n",
        "\n",
        "    print(\"Detailed classification report:\")\n",
        "    print()\n",
        "    print(\"The model is trained on the full development set.\")\n",
        "    print(\"The scores are computed on the full evaluation set.\")\n",
        "    print()\n",
        "    y_true, y_pred = y_test, clf.predict(X_test)\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}