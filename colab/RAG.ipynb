{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kenwkliu/ideas/blob/master/colab/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG (Retrieval Augmented Generation)\n",
        "\n",
        "Based on Shaw Talebi article: https://towardsdatascience.com/how-to-improve-llms-with-rag-abdc132f76ac <br>\n"
      ],
      "metadata": {
        "id": "WWiBVBuOUm-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Execute this block to start importing the libraries (**llama_index**) and helper functions\n",
        "\n",
        "!pip install -q llama-index\n",
        "!pip install -q llama-index-embeddings-huggingface\n",
        "!pip install -q peft\n",
        "!pip install -q auto-gptq\n",
        "!pip install -q optimum\n",
        "!pip install -q bitsandbytes\n",
        "\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor"
      ],
      "metadata": {
        "id": "uFwm7wDSoF3V",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Settings"
      ],
      "metadata": {
        "id": "gm2fzGngqRlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import an embedding model on HuggingFace hub (https://huggingface.co/spaces/mteb/leaderboard)\n",
        "\n",
        "embeddingModel = \"BAAI/bge-small-en-v1.5\" # @param [\"BAAI/bge-small-en-v1.5\", \"thenlper/gte-large\"] {allow-input: true}\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=embeddingModel)\n",
        "\n",
        "Settings.llm = None\n",
        "Settings.chunk_size = 256\n",
        "Settings.chunk_overlap = 25"
      ],
      "metadata": {
        "id": "lBuIj8Xzqb6A",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read and Store Docs into Vector DB"
      ],
      "metadata": {
        "id": "eGXecXc5tPKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# articles available here: {add GitHub repo}\n",
        "documents = SimpleDirectoryReader(\"articles\").load_data()\n",
        "\n",
        "# store docs into vector DB\n",
        "index = VectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "_1pAvTwntWD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Up Search Function"
      ],
      "metadata": {
        "id": "vETeL8CZt_Bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set number of docs to retreive\n",
        "top_k = 3\n",
        "\n",
        "# configure retriever\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=top_k,\n",
        ")\n",
        "\n",
        "# assemble query engine\n",
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)],\n",
        ")"
      ],
      "metadata": {
        "id": "jERXCirtuIKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieve Relevant Docs"
      ],
      "metadata": {
        "id": "ciyNcoKbuS90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# query documents\n",
        "query = \"What is fat-tailedness?\"\n",
        "response = query_engine.query(query)"
      ],
      "metadata": {
        "id": "VoXW6EovuO64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reformat response\n",
        "context = \"Context:\\n\"\n",
        "for i in range(top_k):\n",
        "    context = context + response.source_nodes[i].text + \"\\n\\n\"\n",
        "\n",
        "print(context)"
      ],
      "metadata": {
        "id": "jgpoCg1Dwk_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import LLM"
      ],
      "metadata": {
        "id": "FsTh3OHpwxWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load fine-tuned model from hub\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision=\"main\")\n",
        "\n",
        "config = PeftConfig.from_pretrained(\"shawhin/shawgpt-ft\")\n",
        "model = PeftModel.from_pretrained(model, \"shawhin/shawgpt-ft\")\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "id": "6vlszLofwmpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use LLM"
      ],
      "metadata": {
        "id": "Aajg1MVTzury"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt (no context)\n",
        "intstructions_string = f\"\"\"ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
        "It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. \\\n",
        "ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
        "thus keeping the interaction natural and engaging.\n",
        "\n",
        "Please respond to the following comment.\n",
        "\"\"\"\n",
        "prompt_template = lambda comment: f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''"
      ],
      "metadata": {
        "id": "EdtTeDQQxAHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comment = \"What is fat-tailedness?\"\n",
        "\n",
        "prompt = prompt_template(comment)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "pAqwqAGMz4mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
        "\n",
        "print(tokenizer.batch_decode(outputs)[0])"
      ],
      "metadata": {
        "id": "P-CFTwGa0BEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt (with context)\n",
        "prompt_template_w_context = lambda context, comment: f\"\"\"[INST]ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
        "It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. \\\n",
        "ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
        "thus keeping the interaction natural and engaging.\n",
        "\n",
        "{context}\n",
        "Please respond to the following comment. Use the context above if it is helpful.\n",
        "\n",
        "{comment}\n",
        "[/INST]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xjHEV6FD0I_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template_w_context(context, comment)\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
        "\n",
        "print(tokenizer.batch_decode(outputs)[0])"
      ],
      "metadata": {
        "id": "gAuGtKt81TJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cRUswNvJfqFk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}