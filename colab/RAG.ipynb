{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kenwkliu/ideas/blob/master/colab/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG (Retrieval Augmented Generation)\n",
        "\n",
        "Based on Shaw Talebi article: https://towardsdatascience.com/how-to-improve-llms-with-rag-abdc132f76ac <br>\n"
      ],
      "metadata": {
        "id": "WWiBVBuOUm-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Execute this block to start importing the libraries (**llama_index**) and helper functions\n",
        "\n",
        "!pip install -q llama-index\n",
        "!pip install -q llama-index-embeddings-huggingface\n",
        "!pip install -q peft\n",
        "!pip install -q auto-gptq\n",
        "!pip install -q optimum\n",
        "!pip install -q bitsandbytes\n",
        "\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
        "\n",
        "# Wrap the output text\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)\n",
        "\n",
        "# Download the documents\n",
        "!mkdir data\n",
        "!wget https://www.budget.gov.hk/2024/eng/pdf/e_budget_speech_2024-25.pdf\n",
        "!mv e_budget_speech_2024-25.pdf data\n"
      ],
      "metadata": {
        "id": "uFwm7wDSoF3V",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import an embedding model from HuggingFace hub (https://huggingface.co/spaces/mteb/leaderboard)\n",
        "\n",
        "embeddingModel = \"BAAI/bge-small-en-v1.5\" # @param [\"BAAI/bge-small-en-v1.5\", \"thenlper/gte-large\"] {allow-input: true}\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=embeddingModel)\n",
        "\n",
        "Settings.llm = None\n",
        "Settings.chunk_size = 256\n",
        "Settings.chunk_overlap = 25"
      ],
      "metadata": {
        "id": "lBuIj8Xzqb6A",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Read and Store Docs into Vector DB\n",
        "\n",
        "# articles available here: {add GitHub repo}\n",
        "documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "\n",
        "# store docs into vector DB\n",
        "index = VectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "_1pAvTwntWD0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set Up Search Function\n",
        "\n",
        "# set number of docs to retreive\n",
        "top_k = 3\n",
        "\n",
        "# configure retriever\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=top_k,\n",
        ")\n",
        "\n",
        "# assemble query engine\n",
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)],\n",
        ")"
      ],
      "metadata": {
        "id": "jERXCirtuIKp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Look up relevant content from the embedding\n",
        "\n",
        "query = \"What's the change in property tax in the recent HK Finanical Budget?\" # @param {type:\"string\"}\n",
        "\n",
        "# query documents\n",
        "response = query_engine.query(query)\n",
        "\n",
        "# reformat response\n",
        "context = \"Context:\\n\"\n",
        "for i in range(top_k):\n",
        "    context = context + response.source_nodes[i].text + \"\\n\\n\"\n",
        "\n",
        "print(context)"
      ],
      "metadata": {
        "id": "VoXW6EovuO64",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import LLM"
      ],
      "metadata": {
        "id": "FsTh3OHpwxWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load fine-tuned model from hub\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision=\"main\")\n",
        "\n",
        "config = PeftConfig.from_pretrained(\"shawhin/shawgpt-ft\")\n",
        "model = PeftModel.from_pretrained(model, \"shawhin/shawgpt-ft\")\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "id": "6vlszLofwmpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use LLM"
      ],
      "metadata": {
        "id": "Aajg1MVTzury"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt (no context)\n",
        "intstructions_string = f\"\"\"ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
        "It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. \\\n",
        "ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
        "thus keeping the interaction natural and engaging.\n",
        "\n",
        "Please respond to the following comment.\n",
        "\"\"\"\n",
        "prompt_template = lambda comment: f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''"
      ],
      "metadata": {
        "id": "EdtTeDQQxAHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comment = \"What is fat-tailedness?\"\n",
        "\n",
        "prompt = prompt_template(comment)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "pAqwqAGMz4mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
        "\n",
        "print(tokenizer.batch_decode(outputs)[0])"
      ],
      "metadata": {
        "id": "P-CFTwGa0BEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt (with context)\n",
        "prompt_template_w_context = lambda context, comment: f\"\"\"[INST]ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
        "It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. \\\n",
        "ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
        "thus keeping the interaction natural and engaging.\n",
        "\n",
        "{context}\n",
        "Please respond to the following comment. Use the context above if it is helpful.\n",
        "\n",
        "{comment}\n",
        "[/INST]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xjHEV6FD0I_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template_w_context(context, comment)\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
        "\n",
        "print(tokenizer.batch_decode(outputs)[0])"
      ],
      "metadata": {
        "id": "gAuGtKt81TJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cRUswNvJfqFk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}