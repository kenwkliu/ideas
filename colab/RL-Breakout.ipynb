{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL-Breakout.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rXRigfjSa_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "!pip install keras-rl2\n",
        "#!apt-get install -y cmake zlib1g-dev libjpeg-dev xvfb ffmpeg xorg-dev python-opengl libboost-all-dev libsdl2-dev swig\n",
        "!wget https://github.com/kenwkliu/ideas/raw/master/colab/data/dqn_BreakoutDeterministic-v4_weights_1750000.h5f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxju3dbNS0YL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "import gym\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
        "from rl.callbacks import Callback\n",
        "\n",
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3\n",
        "        img = Image.fromarray(observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)\n",
        "\n",
        "# Subclass to visualize on Jupyter Notebook, inherits from rl.callbacks.Callback Class\n",
        "class Render(Callback):\n",
        "    def on_step_end(self, step, logs={}):\n",
        "        plt.clf()\n",
        "        plt.imshow(env.render(mode='rgb_array'))\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSmRKf0xvRtg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENV_NAME = 'BreakoutDeterministic-v4'\n",
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "# Show possible Actions\n",
        "print(\"Possible Actions:\", env.action_space)\n",
        "print(\"Action : 0=no, 1=start, 2=right, 3=left\")\n",
        "\n",
        "# show the initial env\n",
        "plt.figure(figsize=(16, 10))\n",
        "plt.imshow(env.reset())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-tjFiHMvsyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obs, reward, done, info = env.step(3)\n",
        "\n",
        "print(\"Reward:\",reward, \"  Completed:\",done, \"  Info:\",info)\n",
        "plt.figure(figsize=(16, 10))\n",
        "plt.imshow(obs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y1oTOADDihn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "\n",
        "nb_actions = env.action_space.n\n",
        "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
        "model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "print(model.summary())\n",
        "\n",
        "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
        "processor = AtariProcessor()\n",
        "callbacks = Render()\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
        "                              nb_steps=1000000)\n",
        "\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
        "               processor=processor, nb_steps_warmup=50000, gamma=.99, target_model_update=10000,\n",
        "               train_interval=4, delta_clip=1.)\n",
        "\n",
        "dqn.compile(Adam(lr=.00025), metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7lm_yZIThm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "plt.figure(figsize=(16, 10))\n",
        "dqn.fit(env, nb_steps=3000, log_interval=1000)\n",
        "dqn.test(env, nb_episodes=1, visualize=False, callbacks=[callbacks])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP_plqJixJSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.seed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28xE1xIoUiAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(16, 10))\n",
        "weights_filename = 'dqn_BreakoutDeterministic-v4_weights_1750000.h5f'\n",
        "dqn.load_weights(weights_filename)\n",
        "dqn.test(env, nb_episodes=1, visualize=False, callbacks=[callbacks])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoC31v2FFm1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENV_NAME = 'MsPacman-v0'\n",
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "nb_actions = env.action_space.n\n",
        "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
        "model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "print(model.summary())\n",
        "\n",
        "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
        "processor = AtariProcessor()\n",
        "callbacks = Render()\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
        "                              nb_steps=1000000)\n",
        "\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
        "               processor=processor, nb_steps_warmup=50000, gamma=.99, target_model_update=10000,\n",
        "               train_interval=4, delta_clip=1.)\n",
        "\n",
        "dqn.compile(Adam(lr=.00025), metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzvLc2sOGCch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "plt.figure(figsize=(16, 10))\n",
        "dqn.fit(env, nb_steps=3000, log_interval=1000)\n",
        "dqn.test(env, nb_episodes=1, visualize=False, callbacks=[callbacks])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}