{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataSource.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kenwkliu/ideas/blob/master/colab/DataSource.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dn-rlhPBIO8"
      },
      "source": [
        "# Google colab interactive table\n",
        "%load_ext google.colab.data_table\n",
        "\n",
        "!wget https://raw.githubusercontent.com/kenwkliu/ideas/master/colab/plot.py\n",
        "import plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3x0ZK0M7rZR"
      },
      "source": [
        "# get stocks daily data OHLCV (Open/High/Low/Close/Volume) from Yahoo\n",
        "import pandas_datareader.data as web\n",
        "from datetime import datetime\n",
        "\n",
        "stock = '0293.HK'\n",
        "start = '2021' # accepts strings\n",
        "end = datetime(2021, 4, 29) # or datetime objects\n",
        "\n",
        "stockDf= web.DataReader(stock, 'yahoo', start=start, end=end)\n",
        "#print(stockDf.info())\n",
        "stockDf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXnHd0yFMgP5"
      },
      "source": [
        "# Save the stock dataframe to csv for downloading\n",
        "stockDf.to_csv('stockData.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alUV2h5uazQF"
      },
      "source": [
        "# Plot the OHLCV chart\n",
        "plot.ohlcv(stockDf, 'Date', 'Open', 'High', 'Low', 'Close', 'Volume')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrBx3adTEzT6"
      },
      "source": [
        "# Federal Reserve Economic Data (FRED) for currency exchange rate\n",
        "# https://fred.stlouisfed.org/categories/94\n",
        "\n",
        "rmbUsd = web.DataReader('DEXCHUS', 'fred', start=start, end=end)\n",
        "rmbUsd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF0n-aK38bP3"
      },
      "source": [
        "# Get oil price from Quandl\n",
        "!pip install quandl\n",
        "import quandl\n",
        "\n",
        "oil = quandl.get('EIA/PET_RWTC_D').squeeze()\n",
        "print(oil)\n",
        "oil.plot(lw=2, title='WTI Crude Oil Price')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXnmd5cWCpJJ"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# read csv file from URL\n",
        "# minute-by-minute HSI Index Future in 2014\n",
        "url = 'https://raw.githubusercontent.com/kenwkliu/ideas/master/colab/data/HSI-Future-2014.csv'\n",
        "hsiFuture = pd.read_csv(url)\n",
        "\n",
        "print(\"lines# \", len(hsiFuture))\n",
        "hsiFuture[:1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxicX1KB5upo"
      },
      "source": [
        "# read Excel file from URL\n",
        "# Apple Twitter msg (2016-Apr) \n",
        "url = 'https://raw.githubusercontent.com/kenwkliu/ideas/master/colab/data/appleTweets.xlsx'\n",
        "appleTweets = pd.read_excel(url)\n",
        "\n",
        "# Look at the subset of useful columns \n",
        "COLUMNS = ['Date', 'User Name', 'Tweet content', 'Following', 'Hashtags']\n",
        "appleTweets[COLUMNS][:5000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMWSX8ptoUQT"
      },
      "source": [
        "# Google Trend: https://trends.google.com.hk/trends/trendingsearches/daily?geo=HK\n",
        "\n",
        "# Google trends API\n",
        "# https://pypi.org/project/pytrends/\n",
        "!pip install pytrends"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-mryJPCohFY"
      },
      "source": [
        "import pandas as pd                         \n",
        "from pytrends.request import TrendReq \n",
        "\n",
        "pytrend = TrendReq()\n",
        "pytrend.trending_searches(pn = 'hong_kong')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC-bbTrNC7Pw"
      },
      "source": [
        "pytrend.build_payload(kw_list=['hong kong', '香港'], timeframe='2021-1-1 2021-2-1')\n",
        "related_queries = pytrend.related_queries()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLS-zuZSpdUV"
      },
      "source": [
        "related_queries['hong kong']['top']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JC540-qpqLi"
      },
      "source": [
        "related_queries['hong kong']['rising']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNQ2AjRLAKuz"
      },
      "source": [
        "kw_list=['gme', 'amc']\n",
        "\n",
        "pytrend.get_historical_interest(kw_list, \n",
        "                                 year_start=2020, month_start=12, day_start=1, hour_start=0,\n",
        "                                 year_end=2021, month_end=2, day_end=1, hour_end=0, sleep=0)\n",
        "pytrend.interest_over_time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZldgTXPZaZ-"
      },
      "source": [
        "# Weather forecast from open API in JSON format\n",
        "import pandas as pd\n",
        "import json\n",
        "import requests\n",
        "\n",
        "url = 'https://data.weather.gov.hk/weatherAPI/opendata/weather.php?dataType=fnd&lang=en'\n",
        "data = json.loads(requests.get(url).text)\n",
        "df = pd.DataFrame(data['weatherForecast'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8KwffiQ616E"
      },
      "source": [
        "# read html table\n",
        "import pandas as pd\n",
        "\n",
        "sp_url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
        "sp = pd.read_html(sp_url, header=0)[0] # returns a list for each table\n",
        "print(sp.info())\n",
        "sp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdjcYI8Nqv2s"
      },
      "source": [
        "# webscraping library\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Use Webscraping to extract HK stock Chinese names from wiki web site\n",
        "hk_url = 'https://zh-yue.wikipedia.org/wiki/%E9%A6%99%E6%B8%AF%E4%B8%8A%E5%B8%82%E5%85%AC%E5%8F%B8%E4%B8%80%E8%A6%BD'\n",
        "html = requests.get(hk_url)\n",
        "soup = BeautifulSoup(html.text, 'html.parser')\n",
        "soup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y72TwKdqzsD"
      },
      "source": [
        "# Read the tags line by line and scrape the stock code and names\n",
        "code = []\n",
        "name = []\n",
        "STOCK_SUFFIX = '.HK'\n",
        "\n",
        "a_tags = soup.find(\"div\", attrs={\"id\":\"mw-content-text\"})\n",
        "all_li = a_tags.find_all(\"li\", attrs={\"class\":\"\"})\n",
        "\n",
        "for li in all_li:\n",
        "    content = li.text.strip()\n",
        "    code.append(str(content[:4] + STOCK_SUFFIX))\n",
        "    name.append(content[4:].strip())\n",
        "    \n",
        "chiNames = pd.DataFrame(index=code, data=name)\n",
        "chiNames = chiNames.reset_index()\n",
        "chiNames.columns = [\"code\", \"chiName\"]\n",
        "chiNames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OXJUJ16LFTs"
      },
      "source": [
        "# Example 2: Extract New York Resturant Names\n",
        "# set and request url; extract html source code\n",
        "url = \"https://www.opentable.com/new-york-restaurant-listings\"\n",
        "html = requests.get(url)\n",
        "html.text[:500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oIpk-T9LSwL"
      },
      "source": [
        "# parse raw html => soup object\n",
        "soup = BeautifulSoup(html.text, 'html.parser')\n",
        "\n",
        "# for each span tag, print out text => restaurant name\n",
        "for entry in soup.find_all(name='span', attrs={'class':'rest-row-name-text'}):\n",
        "  print(entry.text)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}