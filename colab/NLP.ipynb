{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-3iNW_x7VE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/kenwkliu/ideas/master/colab/preprocess.py\n",
        "import preprocess\n",
        "\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('treebank')\n",
        "\n",
        "from nltk.corpus import treebank\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import spacy\n",
        "ner = spacy.load('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqhR5wf-7VE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = 'Apple was run by Steve Jobs before 2011 and now it is Tim Cook'\n",
        "\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7CyW0kF7VE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# POS tagging\n",
        "# https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "tagged[0:len(tagged)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv8ROmg475AT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NER (Named Entity Recognition)\n",
        "doc = ner(sentence)\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, \":\", ent.label_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79OTnYzf7VFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stemming by PorterStemmer\n",
        "# https://www.nltk.org/_modules/nltk/stem/porter.html\n",
        "\n",
        "s = PorterStemmer()\n",
        "print(s.stem('Having'))\n",
        "print(s.stem('Have'))\n",
        "print(s.stem('Had'))\n",
        "\n",
        "print(s.stem('Fishing'))\n",
        "print(s.stem('Fish'))\n",
        "print(s.stem('Fisher'))\n",
        "print(s.stem('Fishes'))\n",
        "print(s.stem('Fished'))\n",
        "\n",
        "print(s.stem('am'))\n",
        "print(s.stem('is'))\n",
        "print(s.stem('was'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDsn8Dxp7VFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lemmatization by WordNet\n",
        "# Lemmatization is the process of converting a word to its base form. \n",
        "# Lemmatization considers the context and converts the word to its meaningful base form, \n",
        "# whereas stemming just removes the last few characters\n",
        "# Sometimes the same word can have multiple different lemmas. \n",
        "# Based on the context (by POS tag), extract the appropriate lemma.\n",
        "\n",
        "s = WordNetLemmatizer()\n",
        "print(s.lemmatize('having', pos='v'))\n",
        "print(s.lemmatize('have', pos='v'))\n",
        "print(s.lemmatize('had', pos='v'))\n",
        "\n",
        "print(s.lemmatize('fishing', pos='v'))\n",
        "print(s.lemmatize('fish', pos='v'))\n",
        "print(s.lemmatize('fisher', pos='n'))\n",
        "print(s.lemmatize('fishes', pos='v'))\n",
        "print(s.lemmatize('fished', pos='v'))\n",
        "\n",
        "print(s.lemmatize('am', pos='v'))\n",
        "print(s.lemmatize('is', pos='v'))\n",
        "print(s.lemmatize('was', pos='v'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2_1XHCq7VFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tree Bank\n",
        "words = treebank.words()\n",
        "\n",
        "print(\"Word Count\", len(words))\n",
        "print(words[:17])\n",
        "\n",
        "parsed = treebank.parsed_sents()[0]\n",
        "print(parsed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_QpCZTCABP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bi-grams\n",
        "bigrams = nltk.bigrams(words)\n",
        "biFdist = nltk.FreqDist(bigrams)\n",
        "biFdist.plot(20, cumulative=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om-VDIbYAMJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocess the words\n",
        "preprocessed = []\n",
        "\n",
        "for w in words:\n",
        "  p = preprocess.process(w)\n",
        "  if len(p)>0: preprocessed.append(p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7YfZPJDAQzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bigrams = nltk.bigrams(preprocessed)\n",
        "biFdist = nltk.FreqDist(bigrams)\n",
        "biFdist.plot(20, cumulative=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}