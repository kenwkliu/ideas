{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcsI49qYFkVyH2Vprze0Xl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kenwkliu/ideas/blob/master/colab/llamaIndexReplicate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama_index\n",
        "!pip install -q replicate\n",
        "!pip install -q docx2txt\n",
        "\n",
        "# Get the Replicate API key\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = userdata.get('replicate')\n",
        "\n",
        "# download an doc to the data folder\n",
        "!wget https://github.com/kenwkliu/ideas/raw/master/colab/data/hkuspaceSeminars.docx\n",
        "!mkdir data\n",
        "!mv hkuspaceSeminars.docx data"
      ],
      "metadata": {
        "id": "SExn7bD2IRIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms import Replicate\n",
        "\n",
        "'''\n",
        "value = [0, 1]\n",
        "A high temperature produces more unpredictable and creative results,\n",
        "while a low temperature produces more deterministic and conservative output\n",
        "'''\n",
        "temperature = 0.1\n",
        "\n",
        "llama2_7b_chat = \"meta/llama-2-7b-chat:8e6975e5ed6174911a6ff3d60540dfd4844201974602551e10e9e87ab143d81e\"\n",
        "llm = Replicate(\n",
        "    model=llama2_7b_chat,\n",
        "    temperature=temperature,\n",
        "    additional_kwargs={\"top_p\": 1, \"max_new_tokens\": 300},\n",
        ")"
      ],
      "metadata": {
        "id": "N_TogTqjIwC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings import HuggingFaceEmbedding\n",
        "from llama_index import ServiceContext\n",
        "\n",
        "# https://huggingface.co/BAAI/bge-small-en-v1.5\n",
        "# FlagEmbedding can map any text to a low-dimensional dense vector which can be used for tasks\n",
        "# like retrieval, classification, clustering, or semantic search.\n",
        "# And it also can be used in vector databases for LLMs.\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    llm=llm, embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "id": "OGKny1DmI30F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
        "\n",
        "# llama index read the data folder, embed the content and store in a vector database\n",
        "documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, service_context=service_context\n",
        ")\n",
        "\n",
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "zLcEjmpyI_ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the seminar on 27 Feb 2024\"\n",
        "\n",
        "print(query_engine.query(prompt))"
      ],
      "metadata": {
        "id": "pObwYEZ3J1P5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}